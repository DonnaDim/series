# Overview {#overview}

Welcome. 

This Series is going to be fun and empowering! We will talk about a lot of tools and practices to make your science more streamlined. This is really powerful, cool stuff, and not just for data: I made and published this book using the tools and workflows we'll talk about.

The first half of the Series focuses efficiency and open culture within the lab, and the second half is about sustained learning and bringing these practices to the broader campus community.

## Why we're here

We're all scientists because we want to make a difference. But — Data analysis can be inefficient and demoralizing when you’re without the right tools/skills and you feel alone. But open tools, practices, and communities exist that are powerful and empowering. And we can learn and use open practices for science.

And to make science more of a collaborative effort with everyone valued and able to contribute in their own way.

We are going to introduce you to concepts and tools and how to create a shared culture around them.

Whether it's what to learn or how to help. Will help get you started.


## What to expect

This is going to be a fun course.  

There are no skills required to participate, and we will not be teaching how to code or set up databases. Instead, we will be provide an opportunity to discuss existing tools and how to engage, meet other labs, discuss next steps, and stay accountable.

If you have these files on your computer, this Series is for you. 

- analysis_final_v2.xlsx
- Re:FWD:Fwd:Data question

We'll talk about tools and practices broadly, but also with specific examples using R and GitHub. Yes, software will change and become outdated; it always has. But seeing what is possible and becoming versed in embracing existing architecture and practices can save a lot of time — and will be transferrable skills as the actual software changes. Analogy: if you learn one musical instrument, you will be able to learn another one more fluidly than if you have never learned one to begin with.

The plan is to expose you to a lot of great tools and practices that you can have confidence using in your research. We will expose you to what is possible and help you start planning and also actually incrementally weaving them into your existing workflows. The point is not to overwhelm you or make you feel like it's too late for you or that you would need to throw out and redo everything you've ever done in order to take the first step. No. By seeing what's possible and how shared practices can make your own life easier, and life easier and more streamlined and fun with your lab and beyond, you'll start experimenting with these practices and in a few years you will be working in a completely different way. 
We are going to be discussing a wide range of topics and working to seed habits for you to engage and learn with them with our lab and others on campus. We're going to go through a lot and it's less important that you remember it all. More importantly, you'll know what is possible, have confidence that you can do it, and have allies so you're not alone. The main thing to take away is that there are good ways to approach your analyses; we will teach you to expect that so you can find what you need and use it! A theme throughout is that tools exist and are being developed by real, and extraordinarily nice, people to meet you where you are and help you do what you need to do. If you expect and appreciate that, you will be more efficient in doing your awesome science.

Everyone in this workshop is coming from a different place with different experiences and expectations. But everyone will learn something new here, because there is so much innovation in the data science world. You are encouraged to ask questions and answer those of others. These tools are new to all of us, and the best ideas come from questions from anyone. If you are already familiar with some of this material, think about how your experience was learning it, and how you might teach it to others. Use these workshop materials not only as a reference in the future but also for talking points so you can communicate the importance of these tools to your communities. A big part of this Series is not only for you to learn these skills, but for you to also teach others and increase the value and practice of open data science in science as a whole. 

Vulnerability yes, shame no

Shame is not allowed here. No "I'm 34 and haven't learned GitHub, it's too late for me" or any of that. We have never had the opportunity to learn these things, there should be no shame on your part for that. It takes a lot of time and dedicated effort to learn and employ these practices, and they should be valued and taught. That's why you're here now, you should be proud that you are taking the initiative and your time to do this. No shame.

Vulnerability, however, will be involved in this Series. Vulnerability is a big part of learning and trying new things — this is a safe place for everyone to learn. Vulnerability is taking stock of where you are now and help you map out where you want to be. Being vulnerable is scary. But it shouldn't be lonely: we all have data confessions that would love to talk about and get help with, if only our scientific culture said that was OK; if only we knew how to articulate our questions and have someone to ask. This is a place to share our vulnerabilities to ignite real change. Ask questions. Whether it's a keyboard shortcut or philosophy of data workflows, ask and let's talk about it.

Here is some of my vulnerability: 

> Two years into my PhD program, after trying to understand how to be a scientist, read All The Things that could help me formulate my thesis questions, doing shipboard fieldwork, I finally, in 2008, got my first data back: data archived on an electronic tag that had been attached to a live Humboldt squid swimming freely off the Californian coast. So cool. What was not cool was that that data file was too big to be opened in Excel — the only data analysis tool I knew. Talk about panic and demoralizing despair. Since I had the mindset that I Could Never Learn To Code, I had 2 options. My #1 option was to open this file in a text editor and *cut and paste into separate Excel tabs and do all my timeseries analysis by hand on this raw data in separate Excel tabs*. Would I be expecting to have more squid tags meaning more files of data? Yup, I would do my analyses by hand to all of those too. Open #2 was to quit my PhD program. Thankfully, I ***talked to other people about my data struggles***, and found willing, kind, mentors to help me learn how to code. 

You are all welcome here, please be respectful of one another. We are setting a tone of mutual respect and a space place for learning where we assume good intentions and interact with kindness and empathy. Pass it on. 

## What's possible with open data science (demo)

- github for collaborating (code, text)
- R for automation, visualizations
- bookdown
- websites
- shiny
- github for project management
  - organize by *project*, i.e., keep that code and those methods in same parent folder, rather than all the R code you've ever written being in a giant folder, spanning projects
  - public & private issues, tagging people on commits, kaban board

Live: fix a tpyo and republish the book/page

Personal:
- I don't have all the answers, but I expect them and know how to look
- I build websites with RStudio+GitHub
- Personal website: I link to pubs, slide decks, and interviews/media

## What you'll learn

Seeing what's possible opens up what you expect. There is a bit of a chicken and egg issue here: you need to be exposed to things so you know what's possible and what skills to develop, but you need to kind of know what to look for so you can absorb what you are exposed to. 

Break down that “I teach you learn” model. We are all here to learn and improve. 
Learning horizontally.
Be able to talk to your PI and other scientists like a normal person. 

Expect there's a better way, have agency to find it.

A bit part of this is about expecting there is a better way to do something. And then to have agency to figure out what that is. 

This series is not about micro-managing your science but about providing guidance & structure so that everyone in the lab is not silently struggling to reinvent the wheel and coming up with weird homegrown data approaches.

What skills you should have and what you should be thinking of, along with some of the tools you can use. Will be building out the Resources page on the website for this purpose. And search the blogs. 


## Deliverables

The deliverables for the program are as follows. 
Openscapes Roadmap for your lab (Table 1)
Seaside chats
Study groups
Become Champions – for your labs, departments, communities.

### Assignments

There will be assignments between each call that should take about two hours over two weeks. Assignments are designed to be done during lab "data chats", weekly meetings to discuss data workflows and establish shared practices. 


## Additional reading

Practical Computing for Biologists. Introduction to the Terminal/command line, introduction to regular expressions. Chapter 2 alone. 

